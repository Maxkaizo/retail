{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ddd977",
   "metadata": {},
   "source": [
    "#  Entrenamiento y selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f8b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932efe8",
   "metadata": {},
   "source": [
    "## 1. Cargar dataset y filtrar 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8464f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros totales 2023: 1239531\n",
      "Rango de fechas: 2023-01-01 00:00:00 → 2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(\"../data/sales_train_enriched.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Filtrar solo año 2023\n",
    "df = df[df[\"date\"].dt.year == 2023].copy()\n",
    "\n",
    "print(\"Registros totales 2023:\", len(df))\n",
    "print(\"Rango de fechas:\", df[\"date\"].min(), \"→\", df[\"date\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55572238",
   "metadata": {},
   "source": [
    "## 2. Split temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318fe706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño entrenamiento: 991624\n",
      "Tamaño validación: 247907\n",
      "Última fecha train: 2023-10-21 00:00:00\n",
      "Primera fecha val: 2023-10-21 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Ordenar por fecha (muy importante en series temporales)\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "# Split 80/20 cronológico\n",
    "split_idx = int(len(df) * 0.8)\n",
    "\n",
    "train = df.iloc[:split_idx]\n",
    "val   = df.iloc[split_idx:]\n",
    "\n",
    "# Definir X, y\n",
    "X_train = train.drop(columns=[\"sales\", \"date\"])\n",
    "y_train = train[\"sales\"]\n",
    "\n",
    "X_val = val.drop(columns=[\"sales\", \"date\"])\n",
    "y_val = val[\"sales\"]\n",
    "\n",
    "print(\"Tamaño entrenamiento:\", len(train))\n",
    "print(\"Tamaño validación:\", len(val))\n",
    "print(\"Última fecha train:\", train[\"date\"].max())\n",
    "print(\"Primera fecha val:\", val[\"date\"].min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5adc3e",
   "metadata": {},
   "source": [
    "## 3. Selección de features a usar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e2a61",
   "metadata": {},
   "source": [
    "Seleccionamos las columnas que realmente creemos que pueden aportar información al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4f0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"warehouse\", \"availability\", \"price_log\", \"orders_log\", \"max_discount\",\n",
    "    \"L1_category_name_en\", \"L2_category_name_en\", \"L3_category_name_en\", \"L4_category_name_en\",\n",
    "    \"holiday\", \"shops_closed\", \"winter_school_holidays\", \"school_holidays\", \n",
    "    \"year\", \"day_of_week\", \"day_of_year\", \"year_month\", \"cos_day\", \"sin_day\", \n",
    "    \"sales_rolling_7d\", \"sales_rolling_28d\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fafd2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"sales_log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb68ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer(sparse=True)\n",
    "\n",
    "train_dicts = X_train.to_dict(orient=\"records\")\n",
    "X_train_encoded = dv.fit_transform(train_dicts).astype(\"float32\")\n",
    "\n",
    "val_dicts = X_val.to_dict(orient=\"records\")\n",
    "X_val_encoded = dv.transform(val_dicts).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09957b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[target].values\n",
    "y_val = X_val[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b23f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c223287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/04 00:57:33 INFO mlflow.tracking.fluent: Experiment with name 'retail_nb_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run rf_baseline at: http://localhost:5000/#/experiments/1/runs/46f51917829e423bbbcf4226fdbd25b9\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf_baseline\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Modelo\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[1;32m     13\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     14\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     15\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     16\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m     rf\u001b[38;5;241m.\u001b[39mfit(X_train_encoded, y_train)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Predicciones\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_val_encoded)\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:374\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n\u001b[1;32m    373\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 374\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(\n\u001b[1;32m    375\u001b[0m         X, estimator_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/sklearn/tree/_classes.py:214\u001b[0m, in \u001b[0;36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001b[0;34m(self, X, estimator_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m common_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(estimator_name\u001b[38;5;241m=\u001b[39mestimator_name, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_support_missing_values(X):\n\u001b[0;32m--> 214\u001b[0m     assert_all_finite(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_kwargs)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/sklearn/utils/validation.py:210\u001b[0m, in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_all_finite\u001b[39m(\n\u001b[1;32m    173\u001b[0m     X,\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    178\u001b[0m ):\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Throw a ValueError if X contains NaN or infinity.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    Test failed: Array contains non-finite values.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     _assert_all_finite(\n\u001b[1;32m    211\u001b[0m         X\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X) \u001b[38;5;28;01melse\u001b[39;00m X,\n\u001b[1;32m    212\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    213\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    214\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    121\u001b[0m     X,\n\u001b[1;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/forecasting/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"retail_nb_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"rf_baseline\"):\n",
    "    # Modelo\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=10,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred = rf.predict(X_val_encoded)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # Log params, metrics, model\n",
    "    mlflow.log_param(\"n_estimators\", 10)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Save model locally\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    model_path = \"outputs/rf_model\"\n",
    "    mlflow.sklearn.save_model(rf, model_path)\n",
    "\n",
    "    # Log artifacts manually\n",
    "    mlflow.log_artifacts(model_path, artifact_path=\"model\")\n",
    "\n",
    "print(f\"RMSE on validation: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cac1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's rmse: 0.0304644\tvalid_1's rmse: 0.0356869\n",
      "[100]\ttraining's rmse: 0.0202031\tvalid_1's rmse: 0.0242816\n",
      "[150]\ttraining's rmse: 0.018392\tvalid_1's rmse: 0.0226674\n",
      "[200]\ttraining's rmse: 0.0174329\tvalid_1's rmse: 0.0222356\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's rmse: 0.0174329\tvalid_1's rmse: 0.0222356\n",
      "🏃 View run lgbm_baseline at: http://localhost:5000/#/experiments/4/runs/2e5ee8ec799d46aa817580cb3a0e1d95\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/4\n",
      "✅ RMSE on validation: 0.0222\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lightgbm as lgb\n",
    "\n",
    "with mlflow.start_run(run_name=\"lgbm_baseline\"):\n",
    "    train_set = lgb.Dataset(X_train_encoded, label=y_train)\n",
    "    val_set = lgb.Dataset(X_val_encoded, label=y_val, reference=train_set)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=[train_set, val_set],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_val_encoded, num_iteration=model.best_iteration)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # Log params y métricas\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Guardar modelo local y loguearlo como artifact\n",
    "    os.makedirs(\"outputs_lgbm\", exist_ok=True)\n",
    "    model.save_model(\"outputs_lgbm/lgbm_model.txt\")\n",
    "    mlflow.log_artifact(\"outputs_lgbm/lgbm_model.txt\", artifact_path=\"lgbm_model\")\n",
    "\n",
    "print(f\"✅ RMSE on validation: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d233bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.05897\tval-rmse:1.05428\n",
      "[50]\ttrain-rmse:0.02231\tval-rmse:0.02522\n",
      "[100]\ttrain-rmse:0.01851\tval-rmse:0.02165\n",
      "[150]\ttrain-rmse:0.01769\tval-rmse:0.02121\n",
      "[200]\ttrain-rmse:0.01696\tval-rmse:0.02098\n",
      "[249]\ttrain-rmse:0.01633\tval-rmse:0.02096\n",
      "🏃 View run xgb_baseline at: http://localhost:5000/#/experiments/4/runs/24d62de0af7a430caf81d0663c7ab980\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/4\n",
      "✅ RMSE on validation: 0.0209\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"retail_nb_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_baseline\"):\n",
    "    # Convertir a DMatrix (estructura interna de XGBoost)\n",
    "    dtrain = xgb.DMatrix(X_train_encoded, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val_encoded, label=y_val)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "\n",
    "    # Entrenar con early stopping\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(dval, iteration_range=(0, model.best_iteration))\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    # Log params y métricas\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Guardar modelo local y subirlo a MLflow\n",
    "    os.makedirs(\"outputs_xgb\", exist_ok=True)\n",
    "    model.save_model(\"outputs_xgb/xgb_model.json\")\n",
    "    mlflow.log_artifact(\"outputs_xgb/xgb_model.json\", artifact_path=\"xgb_model\")\n",
    "\n",
    "print(f\"✅ RMSE on validation: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b71f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"retail_nb_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"catboost_baseline\"):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        depth=8,\n",
    "        learning_rate=0.08,\n",
    "        loss_function=\"RMSE\",\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        thread_count=-1\n",
    "    )\n",
    "    # Nota: usamos X_train_encoded / X_val_encoded (ya vectorizados)\n",
    "    model.fit(X_train_encoded, y_train, eval_set=(X_val_encoded, y_val), use_best_model=True)\n",
    "\n",
    "    y_pred = model.predict(X_val_encoded)\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_params({\n",
    "        \"iterations\": 300,\n",
    "        \"depth\": 8,\n",
    "        \"learning_rate\": 0.08,\n",
    "        \"loss_function\": \"RMSE\"\n",
    "    })\n",
    "\n",
    "    os.makedirs(\"outputs_cat\", exist_ok=True)\n",
    "    model.save_model(\"outputs_cat/cat_model.cbm\")  # formato nativo CatBoost\n",
    "    mlflow.log_artifact(\"outputs_cat/cat_model.cbm\", artifact_path=\"catboost_model\")\n",
    "\n",
    "print(f\"✅ CatBoost RMSE on validation: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782960ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
